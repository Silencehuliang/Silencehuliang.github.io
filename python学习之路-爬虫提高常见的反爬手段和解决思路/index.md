# Python学习之路-爬虫提高:常见的反爬手段和解决思路


## 常见的反爬手段和解决思路

### 明确反反爬的主要思路

反反爬的主要思路就是：尽可能的去模拟浏览器，浏览器在如何操作，代码中就如何去实现。浏览器先请求了地址url1，保留了cookie在本地，之后请求地址url2，带上了之前的cookie，代码中也可以这样去实现。

很多时候，爬虫中携带的headers字段，cookie字段，url参数，post的参数很多，不清楚哪些有用，哪些没用的情况下，只能够去尝试，因为每个网站都是不相同的。当然在盲目尝试之前，可以参考别人的思路，我们自己也应该有一套尝试的流程。

### 通过headers字段来反爬

#### 通过headers中的User-Agent字段来反爬

通过User-Agent字段反爬的话，只需要给他在请求之前添加User-Agent即可，更好的方式是使用User-Agent池来解决,我们可以考虑收集一堆User-Agent的方式，或者是随机生成User-Agent

#### 通过referer字段或者是其他字段来反爬

例如豆瓣电视剧中，通过referer字段来反爬，我们只需要添加上即可

#### 通过cookie来反爬

- 如果目标网站不需要登录 每次请求带上前一次返回的cookie，比如requests模块的session
- 如果目标网站需要登录 准备多个账号，通过一个程序获取账号对应的cookie，组成cookie池，其他程序使用这些cookie

### 通过js来反爬

#### 通过js实现跳转来反爬

在请求目标网站的时候，我们看到的似乎就请求了一个网站，然而实际上在成功请求目标网站之前，中间可能有通过js实现的跳转，我们肉眼不可见，这个时候可以通过点击perserve log按钮实现观察页面跳转情况

在这些请求中，如果请求数量很多，一般来讲，只有那些response中带cookie字段的请求是有用的，意味着通过这个请求，对方服务器有设置cookie到本地

#### 通过js生成了请求参数

对应的需要分析js，观察加密的实现过程

#### 通过js实现了数据的加密

对应的需要分析js，观察加密的实现过程

#### 通过验证码来反爬

通过打码平台或者是机器学习的方法识别验证码，其中打码平台廉价易用，更值得推荐

#### 通过ip地址来反爬

同一个ip大量请求了对方服务器，有更大的可能性会被识别为爬虫，对应的通过购买高质量的ip的方式能够结局问题

### 其他的反爬方式

#### 通过自定义字体来反爬

解决思路：切换到手机版

#### 通过css来反爬

解决思路：计算css的偏移

## 打码平台的使用

### 为什么需要了解打码平台的使用

现在很多网站都会使用验证码来进行反爬，所以为了能够更好的获取数据，需要了解如何使用打码平台爬虫中的验证码

### 常见的打码平台

云打码：http://www.yundama.com/

能够解决通用的验证码识别

极验验证码智能识别辅助：http://jiyandoc.c2567.com/

能够解决复杂验证码的识别

### 云打码的使用

下面代码是云打码平台提供，做了个简单修改，只用传入response.content 即可识别图片

### 常见的验证码的种类

#### url地址不变，验证码不变

这是验证码里面非常简单的一种类型，对应的只需要获取验证码的地址，然后请求，通过打码平台识别即可

#### url地址不变，验证码变化

这种验证码的类型是更加常见的一种类型，对于这种验证码，大家需要思考：

> 在登录的过程中，假设我输入的验证码是对的，对方服务器是如何判断当前我输入的验证码是显示在我屏幕上的验证码，而不是其他的验证码呢？

在获取网页的时候，请求验证码，以及提交验证码的时候，对方服务器肯定通过了某种手段验证我之前获取的验证码和最后提交的验证码是同一个验证码，那这个手段是什么手段呢？

很明显，就是通过cookie来实现的，所以对应的，在请求页面，请求验证码，提交验证码的到时候需要保证cookie的一致性，对此可以使用requests.session来解决
