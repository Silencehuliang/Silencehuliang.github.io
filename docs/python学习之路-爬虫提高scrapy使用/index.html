<!DOCTYPE html>
<html lang="zh-CN">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
        <title>Python学习之路-爬虫提高:scrapy使用 - Silence Blog</title><meta name="Description" content="Silence‘s blog"><meta property="og:title" content="Python学习之路-爬虫提高:scrapy使用" />
<meta property="og:description" content="scrapy项目实现流程 创建一个scrapy项目:scrapy startproject mySpider 生成一个爬虫:scrapy genspider itcast &quot;itcast.cn 提取数据:完善spider，使用xpath" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://silencehuliang.github.io/python%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF-%E7%88%AC%E8%99%AB%E6%8F%90%E9%AB%98scrapy%E4%BD%BF%E7%94%A8/" />
<meta property="og:image" content="https://tva1.sinaimg.cn/large/00729CCqgy1gp1qfrpi14j305k05k745.jpg"/>
<meta property="article:published_time" content="2021-05-08T10:25:36+08:00" />
<meta property="article:modified_time" content="2021-05-09T10:25:36+08:00" />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://tva1.sinaimg.cn/large/00729CCqgy1gp1qfrpi14j305k05k745.jpg"/>

<meta name="twitter:title" content="Python学习之路-爬虫提高:scrapy使用"/>
<meta name="twitter:description" content="scrapy项目实现流程 创建一个scrapy项目:scrapy startproject mySpider 生成一个爬虫:scrapy genspider itcast &quot;itcast.cn 提取数据:完善spider，使用xpath"/>
<meta name="application-name" content="LoveIt">
<meta name="apple-mobile-web-app-title" content="LoveIt"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://silencehuliang.github.io/python%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF-%E7%88%AC%E8%99%AB%E6%8F%90%E9%AB%98scrapy%E4%BD%BF%E7%94%A8/" /><link rel="prev" href="https://silencehuliang.github.io/python%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF-%E7%88%AC%E8%99%AB%E6%8F%90%E9%AB%98scrapy%E5%9F%BA%E7%A1%80/" /><link rel="stylesheet" href="/lib/normalize/normalize.min.2e7125082ec8de83a87adc70253c9c23.css" integrity="md5-LnElCC7I3oOoetxwJTycIw=="><link rel="stylesheet" href="/css/style.min.f03a2ec71b16efbe4f24352e542c93a9.css" integrity="md5-8DouxxsW775PJDUuVCyTqQ=="><link rel="stylesheet" href="/lib/fontawesome-free/all.min.76cb46c10b6c0293433b371bae2414b2.css" integrity="md5-dstGwQtsApNDOzcbriQUsg=="><link rel="stylesheet" href="/lib/animate/animate.min.bc1a6a99c43f5ccc97d2d350bde13f74.css" integrity="md5-vBpqmcQ/XMyX0tNQveE/dA=="><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "Python学习之路-爬虫提高:scrapy使用",
        "inLanguage": "zh-CN",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/silencehuliang.github.io\/python%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF-%E7%88%AC%E8%99%AB%E6%8F%90%E9%AB%98scrapy%E4%BD%BF%E7%94%A8\/"
        },"genre": "posts","keywords": "Python学习之路","wordcount":  5733 ,
        "url": "https:\/\/silencehuliang.github.io\/python%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF-%E7%88%AC%E8%99%AB%E6%8F%90%E9%AB%98scrapy%E4%BD%BF%E7%94%A8\/","datePublished": "2021-05-08T10:25:36+08:00","dateModified": "2021-05-09T10:25:36+08:00","publisher": {
            "@type": "Organization",
            "name": "Silence"},"author": {
                "@type": "Person",
                "name": "Silence"
            },"description": ""
    }
    </script></head>
    <body header-desktop="fixed" header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('dark' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'dark' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="Silence Blog"><img
        class="lazyload logo"
        src="/svg/loading.min.svg"
        data-src="https://tva1.sinaimg.cn/large/00729CCqgy1gp1qfrpi14j305k05k745.jpg"
        data-srcset="https://tva1.sinaimg.cn/large/00729CCqgy1gp1qfrpi14j305k05k745.jpg, https://tva1.sinaimg.cn/large/00729CCqgy1gp1qfrpi14j305k05k745.jpg 1.5x, https://tva1.sinaimg.cn/large/00729CCqgy1gp1qfrpi14j305k05k745.jpg 2x"
        data-sizes="auto"
        alt="https://tva1.sinaimg.cn/large/00729CCqgy1gp1qfrpi14j305k05k745.jpg"
        title="https://tva1.sinaimg.cn/large/00729CCqgy1gp1qfrpi14j305k05k745.jpg" /><span id="id-1" class="typeit"></span></a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> 文章 </a><a class="menu-item" href="/tags/"> 标签 </a><a class="menu-item" href="/categories/"> 分类 </a><a class="menu-item" href="/about/"> 关于 </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                    <i class="fas fa-adjust fa-fw"></i>
                </a>
            </div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="Silence Blog"><img
        class="lazyload logo"
        src="/svg/loading.min.svg"
        data-src="https://tva1.sinaimg.cn/large/00729CCqgy1gp1qfrpi14j305k05k745.jpg"
        data-srcset="https://tva1.sinaimg.cn/large/00729CCqgy1gp1qfrpi14j305k05k745.jpg, https://tva1.sinaimg.cn/large/00729CCqgy1gp1qfrpi14j305k05k745.jpg 1.5x, https://tva1.sinaimg.cn/large/00729CCqgy1gp1qfrpi14j305k05k745.jpg 2x"
        data-sizes="auto"
        alt="https://tva1.sinaimg.cn/large/00729CCqgy1gp1qfrpi14j305k05k745.jpg"
        title="https://tva1.sinaimg.cn/large/00729CCqgy1gp1qfrpi14j305k05k745.jpg" /><span id="id-2" class="typeit"></span></a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        取消
                    </a>
                </div><a class="menu-item" href="/posts/" title="">文章</a><a class="menu-item" href="/tags/" title="">标签</a><a class="menu-item" href="/categories/" title="">分类</a><a class="menu-item" href="/about/" title="">关于</a><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                <i class="fas fa-adjust fa-fw"></i>
            </a></div>
    </div>
</header>
<div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
</div>
<main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">目录</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animated flipInX">Python学习之路-爬虫提高:scrapy使用</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="/" title="Author" rel=" author" class="author"><i class="fas fa-user-circle fa-fw"></i>Silence</a></span>&nbsp;<span class="post-category">收录于 <a href="/categories/%E7%88%AC%E8%99%AB/"><i class="far fa-folder fa-fw"></i>爬虫</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="2021-05-08">2021-05-08</time>&nbsp;<i class="fa fa-history" aria-hidden="true"></i>
                2021-05-09
                <i class="fas fa-pencil-alt fa-fw"></i>&nbsp;约 5733 字&nbsp;
                <i class="far fa-clock fa-fw"></i>&nbsp;预计阅读 12 分钟&nbsp;<span id="/python%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF-%E7%88%AC%E8%99%AB%E6%8F%90%E9%AB%98scrapy%E4%BD%BF%E7%94%A8/" class="leancloud_visitors" data-flag-title="Python学习之路-爬虫提高:scrapy使用">
                        <i class="far fa-eye fa-fw"></i>&nbsp;<span class=leancloud-visitors-count></span>&nbsp;次阅读
                    </span>&nbsp;</div>
        </div><div class="details toc" id="toc-static"  kept="">
                <div class="details-summary toc-title">
                    <span>目录</span>
                    <span><i class="details-icon fas fa-angle-right"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#scrapy项目实现流程">scrapy项目实现流程</a>
      <ul>
        <li><a href="#创建scrapy项目">创建scrapy项目</a></li>
        <li><a href="#创建爬虫">创建爬虫</a></li>
        <li><a href="#完善spider">完善spider</a></li>
        <li><a href="#数据传递到pipeline">数据传递到pipeline</a></li>
        <li><a href="#完善pipeline">完善pipeline</a></li>
        <li><a href="#pipeline使用注意点">pipeline使用注意点</a></li>
        <li><a href="#输出日志log的设置">输出日志LOG的设置</a></li>
      </ul>
    </li>
    <li><a href="#scrapy实现翻页请求">scrapy实现翻页请求</a>
      <ul>
        <li><a href="#实现翻页请求">实现翻页请求</a></li>
        <li><a href="#scrapyrequest的更多参数">scrapy.Request的更多参数</a></li>
      </ul>
    </li>
    <li><a href="#定义item">定义Item</a></li>
    <li><a href="#使用item">使用Item</a></li>
    <li><a href="#scrapy的深入使用">scrapy的深入使用</a>
      <ul>
        <li><a href="#scrapy-shell的使用">scrapy shell的使用</a></li>
        <li><a href="#认识scrapy中的setting文件">认识scrapy中的setting文件</a></li>
        <li><a href="#管道中的open_spider和close_spider-的方法">管道中的<code>open_spider</code>和<code>close_spider</code> 的方法</a></li>
      </ul>
    </li>
    <li><a href="#crawlspider类的使用">crawlspider类的使用</a>
      <ul>
        <li><a href="#crawlspider是什么">crawlspider是什么</a></li>
        <li><a href="#认识crawlspider爬虫">认识crawlspider爬虫</a>
          <ul>
            <li><a href="#创建crawlspdier爬虫的命令">创建crawlspdier爬虫的命令</a></li>
            <li><a href="#观察爬虫内的默认内容">观察爬虫内的默认内容</a></li>
            <li><a href="#crawlspider使用的注意点">crawlspider使用的注意点</a></li>
            <li><a href="#crawlspider的补充知识点">crawlspider的补充知识点</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#下载中间件和模拟登陆">下载中间件和模拟登陆</a>
      <ul>
        <li><a href="#scrapy中下载中间件的使用">scrapy中下载中间件的使用</a>
          <ul>
            <li><a href="#使用方法">使用方法</a></li>
          </ul>
        </li>
        <li><a href="#使用scrapy进行模拟登陆">使用scrapy进行模拟登陆</a>
          <ul>
            <li><a href="#回顾之前的模拟登陆的方法">回顾之前的模拟登陆的方法</a></li>
            <li><a href="#scrapy携带cookie进行模拟登陆">scrapy携带cookie进行模拟登陆</a></li>
            <li><a href="#scrapy发送post请求">scrapy发送post请求</a></li>
            <li><a href="#scrapy进行表单提交">scrapy进行表单提交</a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><h2 id="scrapy项目实现流程">scrapy项目实现流程</h2>
<ul>
<li>创建一个scrapy项目:<code>scrapy startproject mySpider</code></li>
<li>生成一个爬虫:<code>scrapy genspider itcast &quot;itcast.cn</code></li>
<li>提取数据:<code>完善spider，使用xpath等方法</code></li>
<li>保存数据:<code>pipeline中保存数据</code></li>
</ul>
<h3 id="创建scrapy项目">创建scrapy项目</h3>
<p>下面以抓取传智师资库来学习scrapy的入门使用：http://www.itcast.cn/channel/teacher.shtml命令：<code>scrapy startproject +&lt;项目名字&gt;</code></p>
<h3 id="创建爬虫">创建爬虫</h3>
<p>命令：<code>scrapy genspider +&lt;爬虫名字&gt; + &lt;允许爬取的域名&gt;</code></p>
<h3 id="完善spider">完善spider</h3>
<p>完善spider即通过方法进行数据的提取等操作</p>
<p>注意：</p>
<ol>
<li><code>response.xpath</code>方法的返回结果是一个类似list的类型，其中包含的是selector对象，操作和列表一样，但是有一些额外的方法</li>
<li><code>extract()</code> 返回一个包含有字符串的列表</li>
<li><code>extract_first()</code> 返回列表中的第一个字符串，列表为空没有返回None</li>
<li>spider中的parse方法必须有</li>
<li>需要抓取的url地址必须属于allowed_domains,但是start_urls中的url地址没有这个限制</li>
<li>启动爬虫的时候注意启动的位置，是在项目路径下启动</li>
</ol>
<h3 id="数据传递到pipeline">数据传递到pipeline</h3>
<p>为什么要使用yield？</p>
<ul>
<li>让整个函数变成一个生成器，有什么好处呢？</li>
<li>遍历这个函数的返回值的时候，挨个把数据读到内存，不会造成内存的瞬间占用过高</li>
<li>python3中的range和python2中的xrange同理</li>
</ul>
<p>注意：</p>
<ul>
<li>yield能够传递的对象只能是：<code>BaseItem</code>,<code>Request</code>,<code>dict</code>,<code>None</code></li>
</ul>
<h3 id="完善pipeline">完善pipeline</h3>
<p>pipeline在settings中能够开启多个，为什么需要开启多个？</p>
<ul>
<li>不同的pipeline可以处理不同爬虫的数据</li>
<li>不同的pipeline能够进行不同的数据处理的操作，比如一个进行数据清洗，一个进行数据的保存</li>
</ul>
<h3 id="pipeline使用注意点">pipeline使用注意点</h3>
<ul>
<li>使用之前需要在settings中开启</li>
<li>pipeline在setting中键表示位置(即pipeline在项目中的位置可以自定义)，值表示距离引擎的远近，越近数据会越先经过</li>
<li>有多个pipeline的时候，process_item的方法必须<code>return item</code>,否则后一个pipeline取到的数据为None值</li>
<li>pipeline中process_item的方法必须有，否则item没有办法接受和处理</li>
<li>process_item方法接受item和spider，其中spider表示当前传递item过来的spider</li>
</ul>
<h3 id="输出日志log的设置">输出日志LOG的设置</h3>
<p>为了让我们自己希望输出到终端的内容能容易看一些，我们可以在setting中设置log级别</p>
<p>在setting中添加一行（全部大写）：<code>LOG_LEVEL = &quot;WARNING”</code></p>
<p>默认终端显示的是debug级别的log信息</p>
<h2 id="scrapy实现翻页请求">scrapy实现翻页请求</h2>
<p>对于要提取所有页面上的数据该怎么办？</p>
<p>回顾requests模块是如何实现翻页请求的：</p>
<ul>
<li>找到下一页的URL地址</li>
<li>调用requests.get(url)</li>
</ul>
<p>思路：</p>
<ol>
<li>找到下一页的url地址</li>
<li>构造url地址的请求，传递给引擎</li>
</ol>
<h3 id="实现翻页请求">实现翻页请求</h3>
<ol>
<li>
<p>使用方法</p>
<p>在获取到url地址之后，可以通过<code>scrapy.Request(url,callback)</code>得到一个request对象，通过yield关键字就可以把这个request对象交给引擎</p>
</li>
<li>
<p>具体使用</p>
<p>添加User-Agent</p>
<p>同时可以再在setting中设置User-Agent：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback"> USER_AGENT = &#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36&#39;
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>通过爬取腾讯招聘的页面的招聘信息,学习如何实现翻页请求</p>
<p>地址：http://hr.tencent.com/position.php</p>
<p>思路分析：</p>
<ol>
<li>获取首页的数据</li>
<li>寻找下一页的地址，进行翻页获取数据</li>
</ol>
</li>
</ol>
<h3 id="scrapyrequest的更多参数">scrapy.Request的更多参数</h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">scrapy.Request(url[,callback,method=&#34;GET&#34;,headers,body,cookies,meta,dont_filter=False])
</code></pre></td></tr></table>
</div>
</div><p>注意：</p>
<ul>
<li>括号中的参数为可选参数</li>
<li>callback：表示当前的url的响应交给哪个函数去处理</li>
<li>meta：实现数据在不同的解析函数中传递，meta默认带有部分数据，比如下载延迟，请求深度等</li>
<li>dont_filter:默认会过滤请求的url地址，即请求过的url地址不会继续被请求，对需要重复请求的url地址可以把它设置为Ture，比如贴吧的翻页请求，页面的数据总是在变化;start_urls中的地址会被反复请求，否则程序不会启动</li>
</ul>
<h2 id="定义item">定义Item</h2>
<p>定义Item的原因：定义item即提前规划好哪些字段需要抓取，scrapy.Field()仅仅是提前占坑，通过item.py能够让别人清楚自己的爬虫是在抓取什么，同时定义好哪些字段是需要抓取的，没有定义的字段不能使用，防止手误</p>
<h2 id="使用item">使用Item</h2>
<p>Item使用之前需要先导入并且实例化，之后的使用方法和使用字典相同</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"> <span class="kn">from</span> <span class="nn">yangguang.items</span> <span class="kn">import</span> <span class="n">YangguangItem</span>
 <span class="n">item</span> <span class="o">=</span> <span class="n">YangguangItem</span><span class="p">()</span> <span class="c1">#实例化</span>
</code></pre></td></tr></table>
</div>
</div><h2 id="scrapy的深入使用">scrapy的深入使用</h2>
<h3 id="scrapy-shell的使用">scrapy shell的使用</h3>
<p>scrapy shell是scrapy提供的一个终端工具，能够通过它查看scrapy中对象的属性和方法，以及测试xpath</p>
<p>使用方法：<code>scrapy shell http://www.itcast.cn/channel/teacher.shtml</code></p>
<p>在终端输入上述命令后，能够进入python的交互式终端</p>
<p>小知识点：</p>
<ul>
<li>response.url：当前响应的url地址</li>
<li>response.request.url：当前响应对应的请求的url地址</li>
<li>response.headers：响应头</li>
<li>response.body：响应体，也就是html代码，默认是byte类型</li>
<li>response.requests.headers：当前响应的请求头</li>
</ul>
<h3 id="认识scrapy中的setting文件">认识scrapy中的setting文件</h3>
<ul>
<li>
<p>为什么项目中需要配置文件</p>
<ul>
<li>在配置文件中存放一些公共变量，在后续的项目中便便修改，注意其中的变量名一般全部大写</li>
</ul>
</li>
<li>
<p>配置文件中的变量使用方法</p>
<ul>
<li>导入即可使用</li>
</ul>
</li>
<li>
<p><code>settings.py</code>中的重点字段和内涵</p>
<ul>
<li><code>USER_AGENT</code> 设置ua</li>
<li><code>ROBOTSTXT_OBEY</code> 是否遵守robots协议，默认是遵守</li>
<li><code>CONCURRENT_REQUESTS</code> 设置并发请求的数量，默认是16个</li>
<li><code>DOWNLOAD_DELAY</code> 下载延迟，默认无延迟</li>
<li><code>COOKIES_ENABLED</code> 是否开启cookie，即每次请求带上前一次的cookie，默认是开启的</li>
<li><code>DEFAULT_REQUEST_HEADERS</code> 设置默认请求头</li>
<li><code>SPIDER_MIDDLEWARES</code> 爬虫中间件，设置过程和管道相同</li>
<li><code>DOWNLOADER_MIDDLEWARES</code> 下载中间件</li>
</ul>
<h3 id="管道中的open_spider和close_spider-的方法">管道中的<code>open_spider</code>和<code>close_spider</code> 的方法</h3>
<p>在管道中，除了必须定义process_item之外，还可以定义两个方法：</p>
<ul>
<li><code>open_spider(spider)</code> :能够在爬虫开启的时候执行一次</li>
<li><code>close_spider(spider)</code> :能够在爬虫关闭的时候执行一次</li>
</ul>
<p>所以，上述方法经常用于爬虫和数据库的交互，在爬虫开启的时候建立和数据库的连接，在爬虫关闭的时候断开和数据库的连接</p>
</li>
</ul>
<h2 id="crawlspider类的使用">crawlspider类的使用</h2>
<h3 id="crawlspider是什么">crawlspider是什么</h3>
<p>回顾之前的代码中，我们有很大一部分时间在寻找下一页的url地址或者是内容的url地址上面，这个过程能更简单一些么？</p>
<p>思路：</p>
<ul>
<li>从response中提取所有的满足规则的url地址</li>
<li>自动的构造自己requests请求，发送给引擎</li>
</ul>
<p>对应的crawlspider就可以实现上述需求，匹配满足条件的url地址，才发送给引擎，同时能够指定callback函数</p>
<h3 id="认识crawlspider爬虫">认识crawlspider爬虫</h3>
<h4 id="创建crawlspdier爬虫的命令">创建crawlspdier爬虫的命令</h4>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">scrapy genspider –t crawl itcast itcast.cn
</code></pre></td></tr></table>
</div>
</div><h4 id="观察爬虫内的默认内容">观察爬虫内的默认内容</h4>
<p>spider中默认生成的内容如下，其中重点在rules中</p>
<ul>
<li>rules是一个元组或者是列表，包含的是Rule对象</li>
<li>Rule表示规则，其中包含<code>LinkExtractor</code>,<code>callback</code>和<code>follow</code></li>
<li><code>LinkExtractor</code>:连接提取器，可以通过正则或者是xpath来进行url地址的匹配</li>
<li><code>callback</code> :表示经过连接提取器提取出来的url地址响应的回调函数，可以没有，没有表示响应不会进行回调函数的处理</li>
<li><code>follow</code>：表示进过连接提取器提取的url地址对应的响应是否还会继续被rules中的规则进行提取，True表示会，Flase表示不会</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">Itcast1Spider</span><span class="p">(</span><span class="n">CrawlSpider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;itcast1&#39;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;itcast.cn&#39;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;http://itcast.cn/&#39;</span><span class="p">]</span>

    <span class="n">rules</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">Rule</span><span class="p">(</span><span class="n">LinkExtractor</span><span class="p">(</span><span class="n">allow</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;Items/&#39;</span><span class="p">),</span> <span class="n">callback</span><span class="o">=</span><span class="s1">&#39;parse_item&#39;</span><span class="p">,</span> <span class="n">follow</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="k">def</span> <span class="nf">parse_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="n">i</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="c1">#使用xpath进行数据的提取或者url地址的提取</span>
        <span class="k">return</span> <span class="n">i</span>
</code></pre></td></tr></table>
</div>
</div><h4 id="crawlspider使用的注意点">crawlspider使用的注意点</h4>
<ul>
<li>可以用命令创建一个crawlspider的模板，也可以手动创建</li>
<li>CrawlSpider中不能再有以parse为名字的数据提取方法，这个方法被CrawlSpider用来实现基础url提取等功能</li>
<li>一个Rule对象接收很多参数，首先第一个是包含url规则得到LinkExtractor对象，常用的还有callback(制定满足规则的url的解析函数的字符串)和follow(response中提取的链接是否需要跟进)</li>
<li>不指定callback函数的请求下，如果follow为True，满足该rule的url还会继续被请求</li>
<li>如果多个Rule都满足某一个url，会从rules中选择第一个满足的进行操作</li>
</ul>
<h4 id="crawlspider的补充知识点">crawlspider的补充知识点</h4>
<p>LindExtractor更多常见参数：</p>
<p>allow：满足括号中&quot;正则表达式&quot;的url会被提取，如果为空，则全部匹配</p>
<p>deny：满足括号中&quot;正则表达式&quot;的url一定不提取(优先级高于allow)</p>
<p>allow_domains：会被提取的链接的domains</p>
<p>deny_domains：一定不会被提取链接的domains</p>
<p>restrict_xpaths：使用xpath表达式，和allow共同作用过滤链接，即xpath满足范围内的url地址会被提取</p>
<p>spiders.Rule常见参数：</p>
<p>link_extractor：是一个Link Extractor对象，用于定义需要提取的链接</p>
<p>callback：从link_extractor中每获取到链接时，参数所指定的值作为回调函数</p>
<p>follow：是一个布尔值，制定了根据该规则从response提取的链接是否需要跟进。如果callback为None，follow默认设置为True，否则默认为False</p>
<p>process_links：指定该spider中那个的函数将会被调用，从link_extractor中获取到链接列表时将会调用该函数，该方法主要用来过滤url</p>
<p>process_request:指定该spider中那个的函数将会被调用，该规则提取到每个request时都会调用该函数，用来过滤request</p>
<h2 id="下载中间件和模拟登陆">下载中间件和模拟登陆</h2>
<h3 id="scrapy中下载中间件的使用">scrapy中下载中间件的使用</h3>
<h4 id="使用方法">使用方法</h4>
<p>编写一个<code>Downloader Middlewares</code>和我们编写一个pipeline一样，定义一个类，然后在setting中开启</p>
<ol>
<li>
<p><code>Downloader Middlewares</code>默认的方法：</p>
<ul>
<li>
<p>process_request(self, request, spider)：</p>
<ul>
<li>当每个request通过下载中间件时，该方法被调用。</li>
<li>返回None值：继续请求</li>
<li>返回Response对象：不在请求，把response返回给引擎</li>
<li>返回Request对象：把request对象交给调度器进行后续的请求</li>
</ul>
</li>
<li>
<p>process_response(self, request, response, spider)：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback"> - 当下载器完成http请求，传递响应给引擎的时候调用
 - 返回Resposne：交给process_response来处理
 - 返回Request对象：交给调取器继续请求
</code></pre></td></tr></table>
</div>
</div></li>
</ul>
</li>
<li>
<p>定义实现随机User-Agent的下载中间件</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"> <span class="k">class</span> <span class="nc">UserAgentMiddleware</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
     <span class="k">def</span> <span class="nf">process_request</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">request</span><span class="p">,</span><span class="n">spider</span><span class="p">):</span>
         <span class="n">agent</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">agents</span><span class="p">)</span>
         <span class="n">request</span><span class="o">.</span><span class="n">headers</span><span class="p">[</span><span class="s1">&#39;User-Agent&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">agent</span>
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>定义实现随机使用代理的下载中间件</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback"> class ProxyMiddleware(object):
     def process_request(self,request,spider):
         proxy = random.choice(proxies)
         request.meta[&#39;proxy&#39;] = proxy
</code></pre></td></tr></table>
</div>
</div><p>User-Agent池在这里</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback"> ```python
 USER_AGENTS = [ &#34;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)&#34;, &#34;Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)&#34;, &#34;Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)&#34;, &#34;Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)&#34;, &#34;Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6&#34;, &#34;Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1&#34;, &#34;Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0&#34;, &#34;Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5&#34; ]
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">
</code></pre></td></tr></table>
</div>
</div></li>
</ol>
<h3 id="使用scrapy进行模拟登陆">使用scrapy进行模拟登陆</h3>
<h4 id="回顾之前的模拟登陆的方法">回顾之前的模拟登陆的方法</h4>
<ol>
<li>requests是如何模拟登陆的？
<ol>
<li>直接携带cookies请求页面</li>
<li>找接口发送post请求存储cookie</li>
</ol>
</li>
<li>selenium是如何模拟登陆的？
<ol>
<li>找到对应的input标签，输入文字点击登录</li>
</ol>
</li>
</ol>
<p>scrapy来说，有两个方法模拟登陆：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">1、直接携带cookie
2、找到发送post请求的url地址，带上信息，发送请求
</code></pre></td></tr></table>
</div>
</div><h4 id="scrapy携带cookie进行模拟登陆">scrapy携带cookie进行模拟登陆</h4>
<ol>
<li>
<p>携带cookie进行模拟登陆应用场景：</p>
<ol>
<li>cookie过期时间很长，常见于一些不规范的网站</li>
<li>能在cookie过期之前把搜有的数据拿到</li>
<li>配合其他程序使用，比如其使用selenium把登陆之后的cookie获取到保存到本地，scrapy发送请求之前先读取本地cookie</li>
</ol>
</li>
<li>
<p>scrapy的start_requests方法的学习</p>
<p>scrapy中start_url是通过start_requests来进行处理的，其实现代码如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"> <span class="k">def</span> <span class="nf">start_requests</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
     <span class="bp">cls</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span>
     <span class="k">if</span> <span class="n">method_is_overridden</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">Spider</span><span class="p">,</span> <span class="s1">&#39;make_requests_from_url&#39;</span><span class="p">):</span>
         <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
             <span class="s2">&#34;Spider.make_requests_from_url method is deprecated; it &#34;</span>
             <span class="s2">&#34;won&#39;t be called in future Scrapy releases. Please &#34;</span>
             <span class="s2">&#34;override Spider.start_requests method instead (see </span><span class="si">%s</span><span class="s2">.</span><span class="si">%s</span><span class="s2">).&#34;</span> <span class="o">%</span> <span class="p">(</span>
                 <span class="bp">cls</span><span class="o">.</span><span class="vm">__module__</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="vm">__name__</span>
             <span class="p">),</span>
         <span class="p">)</span>
         <span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_urls</span><span class="p">:</span>
             <span class="k">yield</span> <span class="bp">self</span><span class="o">.</span><span class="n">make_requests_from_url</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
     <span class="k">else</span><span class="p">:</span>
         <span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_urls</span><span class="p">:</span>
             <span class="k">yield</span> <span class="n">Request</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">dont_filter</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><p>所以对应的，如果start_url地址中的url是需要登录后才能访问的url地址，则需要重写<code>start_request</code>方法并在其中手动添加上cookie</p>
</li>
</ol>
<p>在settings中开启cookie_debug</p>
<p>在settings.py中通过设置<code>COOKIES_DEBUG=TRUE</code> 能够在终端看到cookie的传递传递过程</p>
<h4 id="scrapy发送post请求">scrapy发送post请求</h4>
<ol>
<li>
<p>scrapy中发送post请求的方法 通过<code>scrapy.FormRequest</code>能够发送post请求，同时需要添加<code>fromdata</code>参数作为请求体，以及<code>callback</code></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"> <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">FormRequest</span><span class="p">(</span>
             <span class="s2">&#34;https://github.com/session&#34;</span><span class="p">,</span>
             <span class="n">formdata</span><span class="o">=</span><span class="p">{</span>
                 <span class="s2">&#34;authenticity_token&#34;</span><span class="p">:</span><span class="n">authenticity_token</span><span class="p">,</span>
                 <span class="s2">&#34;utf8&#34;</span><span class="p">:</span><span class="n">utf8</span><span class="p">,</span>
                 <span class="s2">&#34;commit&#34;</span><span class="p">:</span><span class="n">commit</span><span class="p">,</span>
                 <span class="s2">&#34;login&#34;</span><span class="p">:</span><span class="s2">&#34;noobpythoner&#34;</span><span class="p">,</span>
                 <span class="s2">&#34;password&#34;</span><span class="p">:</span><span class="s2">&#34;zhoudawei123&#34;</span>
             <span class="p">},</span>
             <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse_login</span>
         <span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>使用scrapy模拟登陆github</p>
<p>思路分析</p>
<ol>
<li>
<p>找到post的url地址</p>
<p>点击登录按钮进行抓包，然后定位url地址为<code>https://github.com/session</code></p>
</li>
<li>
<p>找到请求体的规律</p>
<p>分析post请求的请求体，其中包含的参数均在前一次的响应中</p>
</li>
<li>
<p>验证是否登录成功</p>
<p>通过请求个人主页，观察是否包含用户名</p>
<p>代码实现如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1">#spider/github.py</span>
<span class="c1"># -*- coding: utf-8 -*-</span>
<span class="kn">import</span> <span class="nn">scrapy</span>
<span class="kn">import</span> <span class="nn">re</span>
      
<span class="k">class</span> <span class="nc">GithubSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
 <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;github&#39;</span>
 <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;github.com&#39;</span><span class="p">]</span>
 <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;https://github.com/login&#39;</span><span class="p">]</span>
      
 <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
     <span class="n">authenticity_token</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&#34;//input[@name=&#39;authenticity_token&#39;]/@value&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">extract_first</span><span class="p">()</span>
     <span class="n">utf8</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&#34;//input[@name=&#39;utf8&#39;]/@value&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">extract_first</span><span class="p">()</span>
     <span class="n">commit</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&#34;//input[@name=&#39;commit&#39;]/@value&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">extract_first</span><span class="p">()</span>
      
     <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">FormRequest</span><span class="p">(</span>
         <span class="s2">&#34;https://github.com/session&#34;</span><span class="p">,</span>
         <span class="n">formdata</span><span class="o">=</span><span class="p">{</span>
             <span class="s2">&#34;authenticity_token&#34;</span><span class="p">:</span><span class="n">authenticity_token</span><span class="p">,</span>
             <span class="s2">&#34;utf8&#34;</span><span class="p">:</span><span class="n">utf8</span><span class="p">,</span>
             <span class="s2">&#34;commit&#34;</span><span class="p">:</span><span class="n">commit</span><span class="p">,</span>
             <span class="s2">&#34;login&#34;</span><span class="p">:</span><span class="s2">&#34;noobpythoner&#34;</span><span class="p">,</span>
             <span class="s2">&#34;password&#34;</span><span class="p">:</span><span class="s2">&#34;***&#34;</span>
         <span class="p">},</span>
         <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse_login</span>
     <span class="p">)</span>
      
 <span class="k">def</span> <span class="nf">parse_login</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">response</span><span class="p">):</span>
     <span class="n">ret</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="s2">&#34;noobpythoner&#34;</span><span class="p">,</span><span class="n">response</span><span class="o">.</span><span class="n">text</span><span class="p">,</span><span class="n">re</span><span class="o">.</span><span class="n">I</span><span class="p">)</span>
     <span class="k">print</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div></li>
</ol>
</li>
</ol>
<h4 id="scrapy进行表单提交">scrapy进行表单提交</h4>
<ol>
<li>
<p>方法介绍</p>
<p>scrapy中具有一个方法：<code>scrapy.Formrequest.from_response</code>能够自动的从响应中寻找form表单，然后把formdata中的数据提交到action对应的url地址中</p>
<p>使用实例如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python">  <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
     <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">FormRequest</span><span class="o">.</span><span class="n">from_response</span><span class="p">(</span>
         <span class="n">response</span><span class="p">,</span><span class="c1">#自动的从中寻找action对应的url地址</span>
         <span class="n">formdata</span><span class="o">=</span><span class="p">{</span>
             <span class="s2">&#34;login&#34;</span><span class="p">:</span><span class="s2">&#34;noobpythoner&#34;</span><span class="p">,</span>
             <span class="s2">&#34;password&#34;</span><span class="p">:</span><span class="s2">&#34;***&#34;</span>
         <span class="p">},</span>
         <span class="n">callback</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse_login</span>
     <span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>使用<code>scrapy.Formrequest.from_response</code>进行模拟登陆github</p>
</li>
</ol>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>更新于 2021-05-09</span>
            </div>
            <div class="post-info-license"></div>
        </div>
        <div class="post-info-line">
            <div class="post-info-md"><span>
                            <a class="link-to-markdown" href="/python%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF-%E7%88%AC%E8%99%AB%E6%8F%90%E9%AB%98scrapy%E4%BD%BF%E7%94%A8/index.md" target="_blank">阅读原始文档</a>
                        </span></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="分享到 Twitter" data-sharer="twitter" data-url="https://silencehuliang.github.io/python%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF-%E7%88%AC%E8%99%AB%E6%8F%90%E9%AB%98scrapy%E4%BD%BF%E7%94%A8/" data-title="Python学习之路-爬虫提高:scrapy使用" data-hashtags="Python学习之路"><i class="fab fa-twitter fa-fw"></i></a><a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="https://silencehuliang.github.io/python%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF-%E7%88%AC%E8%99%AB%E6%8F%90%E9%AB%98scrapy%E4%BD%BF%E7%94%A8/" data-hashtag="Python学习之路"><i class="fab fa-facebook-square fa-fw"></i></a><a href="javascript:void(0);" title="分享到 WhatsApp" data-sharer="whatsapp" data-url="https://silencehuliang.github.io/python%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF-%E7%88%AC%E8%99%AB%E6%8F%90%E9%AB%98scrapy%E4%BD%BF%E7%94%A8/" data-title="Python学习之路-爬虫提高:scrapy使用" data-web><i class="fab fa-whatsapp fa-fw"></i></a><a href="javascript:void(0);" title="分享到 Line" data-sharer="line" data-url="https://silencehuliang.github.io/python%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF-%E7%88%AC%E8%99%AB%E6%8F%90%E9%AB%98scrapy%E4%BD%BF%E7%94%A8/" data-title="Python学习之路-爬虫提高:scrapy使用"><i data-svg-src="/lib/simple-icons/icons/line.min.svg"></i></a><a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="https://silencehuliang.github.io/python%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF-%E7%88%AC%E8%99%AB%E6%8F%90%E9%AB%98scrapy%E4%BD%BF%E7%94%A8/" data-title="Python学习之路-爬虫提高:scrapy使用"><i class="fab fa-weibo fa-fw"></i></a><a href="javascript:void(0);" title="分享到 Myspace" data-sharer="myspace" data-url="https://silencehuliang.github.io/python%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF-%E7%88%AC%E8%99%AB%E6%8F%90%E9%AB%98scrapy%E4%BD%BF%E7%94%A8/" data-title="Python学习之路-爬虫提高:scrapy使用" data-description=""><i data-svg-src="/lib/simple-icons/icons/myspace.min.svg"></i></a><a href="javascript:void(0);" title="分享到 Blogger" data-sharer="blogger" data-url="https://silencehuliang.github.io/python%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF-%E7%88%AC%E8%99%AB%E6%8F%90%E9%AB%98scrapy%E4%BD%BF%E7%94%A8/" data-title="Python学习之路-爬虫提高:scrapy使用" data-description=""><i class="fab fa-blogger fa-fw"></i></a><a href="javascript:void(0);" title="分享到 Evernote" data-sharer="evernote" data-url="https://silencehuliang.github.io/python%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF-%E7%88%AC%E8%99%AB%E6%8F%90%E9%AB%98scrapy%E4%BD%BF%E7%94%A8/" data-title="Python学习之路-爬虫提高:scrapy使用"><i class="fab fa-evernote fa-fw"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw"></i>&nbsp;<a href="/tags/python%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/">Python学习之路</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/python%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF-%E7%88%AC%E8%99%AB%E6%8F%90%E9%AB%98scrapy%E5%9F%BA%E7%A1%80/" class="prev" rel="prev" title="Python学习之路-爬虫提高:scrapy基础"><i class="fas fa-angle-left fa-fw"></i>Python学习之路-爬虫提高:scrapy基础</a></div>
</div>
<div id="comments"><div id="valine" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://valine.js.org/">Valine</a>.
            </noscript></div></article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">由 <a href="https://gohugo.io/" target="_blank" rel="noopener noreffer" title="Hugo 0.74.3">Hugo</a> 强力驱动 | 主题 - <a href="https://github.com/dillonzq/LoveIt" target="_blank" rel="noopener noreffer" title="LoveIt 0.2.10"><i class="far fa-kiss-wink-heart fa-fw"></i> LoveIt</a>
                </div><div class="footer-line"><i class="far fa-copyright fa-fw"></i><span itemprop="copyrightYear">2018 - 2021</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/" target="_blank">Silence</a></span>&nbsp;|&nbsp;<span class="license"><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="回到顶部">
                <i class="fas fa-arrow-up fa-fw"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="查看评论">
                <i class="fas fa-comment fa-fw"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/valine/valine.min.css"><link rel="stylesheet" href="/lib/katex/katex.min.4afae7d446ba01ebf7fff19893eaf3a1.css" integrity="md5-Svrn1Ea6Aev3//GYk&#43;rzoQ=="><link rel="stylesheet" href="/lib/katex/copy-tex.min.b1994d1b9785dd8801dbf655df7bf6d9.css" integrity="md5-sZlNG5eF3YgB2/ZV33v22Q=="><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.acf82ee47549fdc386d02768992a49ad.css" integrity="md5-rPgu5HVJ/cOG0CdomSpJrQ=="><script type="text/javascript" src="/lib/valine/Valine.min.b1d2c9b89c70dbf0a8541bfd36afafa1.js" integrity="md5-sdLJuJxw2/CoVBv9Nq&#43;voQ=="></script><script type="text/javascript" src="/lib/smooth-scroll/smooth-scroll.min.100efdceabf7a138f3297e437d078f74.js" integrity="md5-EA79zqv3oTjzKX5DfQePdA=="></script><script type="text/javascript" src="/lib/autocomplete/autocomplete.min.ae7c5011def46b283696baf367586b5d.js" integrity="md5-rnxQEd70ayg2lrrzZ1hrXQ=="></script><script type="text/javascript" src="/lib/lunr/lunr.min.a08f2562fde4550269dfc13028ed2502.js" integrity="md5-oI8lYv3kVQJp38EwKO0lAg=="></script><script type="text/javascript" src="/lib/lunr/lunr.stemmer.support.min.b82fb8d93ce0ea6a485dfa3a0b1e7573.js" integrity="md5-uC&#43;42Tzg6mpIXfo6Cx51cw=="></script><script type="text/javascript" src="/lib/lunr/lunr.zh.min.f1ba3e22560e2eb7474f28feb4507617.js" integrity="md5-8bo&#43;IlYOLrdHTyj&#43;tFB2Fw=="></script><script type="text/javascript" src="/lib/lazysizes/lazysizes.min.b80e49640d4794d4333d00db76ea22f7.js" integrity="md5-uA5JZA1HlNQzPQDbduoi9w=="></script><script type="text/javascript" src="/lib/clipboard/clipboard.min.af8ab36589315582ccdd82f22e84bffb.js" integrity="md5-r4qzZYkxVYLM3YLyLoS/&#43;w=="></script><script type="text/javascript" src="/lib/sharer/sharer.min.c3c3372b4cf5c56dd4e5a4be8ada86c9.js" integrity="md5-w8M3K0z1xW3U5aS&#43;itqGyQ=="></script><script type="text/javascript" src="/lib/typeit/typeit.min.b5f3481ebeaa3e56a2f5f96ea648bf69.js" integrity="md5-tfNIHr6qPlai9flupki/aQ=="></script><script type="text/javascript" src="/lib/katex/katex.min.c158c9e823b681cf535f46596b5e4eac.js" integrity="md5-wVjJ6CO2gc9TX0ZZa15OrA=="></script><script type="text/javascript" src="/lib/katex/auto-render.min.28cd0b98cd3f4fa37d52f3ffe47ad9d4.js" integrity="md5-KM0LmM0/T6N9UvP/5HrZ1A=="></script><script type="text/javascript" src="/lib/katex/copy-tex.min.bfaec7d1dea915d74a7a6d833f0ff62e.js" integrity="md5-v67H0d6pFddKem2DPw/2Lg=="></script><script type="text/javascript" src="/lib/katex/mhchem.min.1bbb252363e83547d4b2186a41eaca28.js" integrity="md5-G7slI2PoNUfUshhqQerKKA=="></script><script type="text/javascript" src="/lib/cookieconsent/cookieconsent.min.4a48532bf0b17c058b8b6854f49de23f.js" integrity="md5-SkhTK/CxfAWLi2hU9J3iPw=="></script><script type="text/javascript">window.config={"code":{"copyTitle":"复制到剪贴板","maxShownLines":10},"comment":{"valine":{"appId":"c4VA4R1Xg3drBKSQzNfWnWlF-gzGzoHsz","appKey":"upEyt7esgWCbm0L7EPDH6bhV","avatar":"mp","el":"#valine","emojiCDN":"https://cdn.jsdelivr.net/npm/emoji-datasource-google@5.0.1/img/google/64/","emojiMaps":{"100":"1f4af.png","alien":"1f47d.png","anger":"1f4a2.png","angry":"1f620.png","anguished":"1f627.png","astonished":"1f632.png","black_heart":"1f5a4.png","blue_heart":"1f499.png","blush":"1f60a.png","bomb":"1f4a3.png","boom":"1f4a5.png","broken_heart":"1f494.png","brown_heart":"1f90e.png","clown_face":"1f921.png","cold_face":"1f976.png","cold_sweat":"1f630.png","confounded":"1f616.png","confused":"1f615.png","cry":"1f622.png","crying_cat_face":"1f63f.png","cupid":"1f498.png","dash":"1f4a8.png","disappointed":"1f61e.png","disappointed_relieved":"1f625.png","dizzy":"1f4ab.png","dizzy_face":"1f635.png","drooling_face":"1f924.png","exploding_head":"1f92f.png","expressionless":"1f611.png","face_vomiting":"1f92e.png","face_with_cowboy_hat":"1f920.png","face_with_hand_over_mouth":"1f92d.png","face_with_head_bandage":"1f915.png","face_with_monocle":"1f9d0.png","face_with_raised_eyebrow":"1f928.png","face_with_rolling_eyes":"1f644.png","face_with_symbols_on_mouth":"1f92c.png","face_with_thermometer":"1f912.png","fearful":"1f628.png","flushed":"1f633.png","frowning":"1f626.png","ghost":"1f47b.png","gift_heart":"1f49d.png","green_heart":"1f49a.png","grimacing":"1f62c.png","grin":"1f601.png","grinning":"1f600.png","hankey":"1f4a9.png","hear_no_evil":"1f649.png","heart":"2764-fe0f.png","heart_decoration":"1f49f.png","heart_eyes":"1f60d.png","heart_eyes_cat":"1f63b.png","heartbeat":"1f493.png","heartpulse":"1f497.png","heavy_heart_exclamation_mark_ornament":"2763-fe0f.png","hole":"1f573-fe0f.png","hot_face":"1f975.png","hugging_face":"1f917.png","hushed":"1f62f.png","imp":"1f47f.png","innocent":"1f607.png","japanese_goblin":"1f47a.png","japanese_ogre":"1f479.png","joy":"1f602.png","joy_cat":"1f639.png","kiss":"1f48b.png","kissing":"1f617.png","kissing_cat":"1f63d.png","kissing_closed_eyes":"1f61a.png","kissing_heart":"1f618.png","kissing_smiling_eyes":"1f619.png","laughing":"1f606.png","left_speech_bubble":"1f5e8-fe0f.png","love_letter":"1f48c.png","lying_face":"1f925.png","mask":"1f637.png","money_mouth_face":"1f911.png","nauseated_face":"1f922.png","nerd_face":"1f913.png","neutral_face":"1f610.png","no_mouth":"1f636.png","open_mouth":"1f62e.png","orange_heart":"1f9e1.png","partying_face":"1f973.png","pensive":"1f614.png","persevere":"1f623.png","pleading_face":"1f97a.png","pouting_cat":"1f63e.png","purple_heart":"1f49c.png","rage":"1f621.png","relaxed":"263a-fe0f.png","relieved":"1f60c.png","revolving_hearts":"1f49e.png","right_anger_bubble":"1f5ef-fe0f.png","robot_face":"1f916.png","rolling_on_the_floor_laughing":"1f923.png","scream":"1f631.png","scream_cat":"1f640.png","see_no_evil":"1f648.png","shushing_face":"1f92b.png","skull":"1f480.png","skull_and_crossbones":"2620-fe0f.png","sleeping":"1f634.png","sleepy":"1f62a.png","slightly_frowning_face":"1f641.png","slightly_smiling_face":"1f642.png","smile":"1f604.png","smile_cat":"1f638.png","smiley":"1f603.png","smiley_cat":"1f63a.png","smiling_face_with_3_hearts":"1f970.png","smiling_imp":"1f608.png","smirk":"1f60f.png","smirk_cat":"1f63c.png","sneezing_face":"1f927.png","sob":"1f62d.png","space_invader":"1f47e.png","sparkling_heart":"1f496.png","speak_no_evil":"1f64a.png","speech_balloon":"1f4ac.png","star-struck":"1f929.png","stuck_out_tongue":"1f61b.png","stuck_out_tongue_closed_eyes":"1f61d.png","stuck_out_tongue_winking_eye":"1f61c.png","sunglasses":"1f60e.png","sweat":"1f613.png","sweat_drops":"1f4a6.png","sweat_smile":"1f605.png","thinking_face":"1f914.png","thought_balloon":"1f4ad.png","tired_face":"1f62b.png","triumph":"1f624.png","two_hearts":"1f495.png","unamused":"1f612.png","upside_down_face":"1f643.png","weary":"1f629.png","white_frowning_face":"2639-fe0f.png","white_heart":"1f90d.png","wink":"1f609.png","woozy_face":"1f974.png","worried":"1f61f.png","yawning_face":"1f971.png","yellow_heart":"1f49b.png","yum":"1f60b.png","zany_face":"1f92a.png","zipper_mouth_face":"1f910.png","zzz":"1f4a4.png"},"enableQQ":false,"highlight":true,"lang":"zh-cn","meta":["nick","mail"],"pageSize":10,"placeholder":"说点什么吧...","recordIP":true,"visitor":true}},"cookieconsent":{"content":{"dismiss":"同意","link":"了解更多","message":"本网站使用 Cookies 来改善您的浏览体验."},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"data":{"id-1":"Silence’s Blog","id-2":"Silence’s Blog"},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"highlightTag":"em","lunrIndexURL":"/index.json","lunrLanguageCode":"zh","lunrSegmentitURL":"/lib/lunr/lunr.segmentit.js","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"lunr"},"typeit":{"cursorChar":"|","cursorSpeed":1000,"data":{"id-1":["id-1"],"id-2":["id-2"]},"duration":-1,"speed":100}};</script><script type="text/javascript" src="/js/theme.min.7bc7989e979c4a19d97db3ab311a80fe.js" integrity="md5-e8eYnpecShnZfbOrMRqA/g=="></script></body>
</html>
